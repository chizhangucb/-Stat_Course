\documentclass[11pt]{article}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx, tabularx, float}
\usepackage{hyperref}

\title{Principal Component Analysis \\
        \large Linear Dimension Reduction Technique}
\date{}

\begin{document}
\maketitle

PCA performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized.

\section{Representations of Individuals}
1. For the standardized data, $\frac{1}{n-1}{\bf X^T}{\bf X}$ is the correlation matrix. \\ (If not standardized, $\frac{1}{n-1}{\bf X^T}{\bf X}$ is the covariance matrix.) \\

\hspace{-0.68cm} 2. $\bf Loadings$: ${\bf V}$ is the $p \times r$ unit-norm matrix with eigenvectors of $\frac{1}{n-1}{\bf X^T}{\bf X}$.
\begin{itemize}
	\item $\frac{1}{n-1}{\bf X^T}{\bf X} = {\bf V}{\bf D}^2{\bf V^T} = {\bf V}{\bf \Lambda}{\bf V^T}$, where 
	\item Each $\bf v_k$ reflects how much each variable $\bf X_j$ loads in the PC $k$. 
\end{itemize}

\hspace{-0.68cm} 3. \textbf{Eigenvalues}
\begin{itemize}
	\item ${\bf D}$ is the $r \times r$ diagonal matrix of $\frac{1}{n-1}{\bf X}$ consisting of $r$ positive eigenvalues (descending ordered).
	\item ${\bf \Lambda}$ is the $r \times r$ diagonal matrix of $\frac{1}{n-1}{\bf X^T}{\bf X}$, where each $\lambda_k$ captures the projected inertia (i.e. variation) on each extracted dimension.
	\item ${\bf \Lambda} = {\bf D}^2$.
	\item $r$ is the rank of $\bf X$.
	\item When $X$ is standardized, $\sum_{k=1}^r\lambda_k = p$, and the proportion of of variation captured by each PC is $\lambda_k/p$.
\end{itemize}

\hspace{-0.68cm} 4. \textbf{Scores} or \textbf{Principal Components}: $\bf Z$ is the $n \times r$ matrix in which each column is the projection of all points on that axis/ PC.
\begin{itemize}
	\item ${\bf Z} = {\bf XV}$
	\item PCs are linear combinations of the variables
\end{itemize}

\section{Looking at Variables}

1. To project the cloud of standardized variables, the projection of variable $j$ onto an axis $k$, is equal to the cosine of the angle $\theta_{jk}$, so this criteria maximizes $\sum_{i=1}^p cos^2(\theta_{jk}) = \sum_{i=1}^p corr^2({\bf x_j}, {\bf z_k})$. \\

\hspace{-0.68cm} 2. \textbf{Variable Factors}: $\bf Q$ is the $p \times r$ matrix in which each column is a linear combination of the objects. 
\begin{itemize}
	\item Projection of variable $j$ on axis $H_1$ generated by vector $\mathbf{u_1}$ is ${\bf x_j^T}{\bf u_1}$.
	\item ${\bf Q} = {\bf X^TU}$
\end{itemize}

\hspace{-0.68cm} 3. \textbf{Eigenvectors}: $\bf U$ is the $p \times k$ matrix.
\begin{itemize}
	\item Each column $\bf u_1$ is an eigenvector of $\frac{1}{n-1}{\bf X}{\bf X^T}$
	\item $\frac{1}{n-1}{\bf X}{\bf X^T}$ is the matrix of inner products between individuals
	\item $\frac{1}{n-1}{\bf X}{\bf X^T} = {\bf U\Lambda U^T} = {\bf U}{\bf D}^2{\bf U^T}$. 
\end{itemize}

\section{Relationship between the representations of Individuals and Variables}

1. $\frac{1}{\sqrt{n-1}}{\bf X} = {\bf UDV^T} \Rightarrow \frac{1}{\sqrt{n-1}}{\bf XV} = {\bf UDV^TV} = {\bf UD} \Rightarrow {\bf U} = \frac{1}{\sqrt{n-1}}{\bf XVD^{-1}}$.

\hspace{-0.68cm} Since ${\bf Z} = {\bf XV}$, ${\bf U} = \frac{1}{\sqrt{n-1}}{\bf ZD^{-1}}$ \\

\hspace{-0.68cm} 2. $\frac{1}{\sqrt{n-1}}{\bf X} = {\bf UDV^T} \Rightarrow \frac{1}{\sqrt{n-1}}{\bf U^TX} = {\bf U^TUDV^T} = {\bf DV^T} \Rightarrow \\
{\bf V} = \frac{1}{\sqrt{n-1}}{\bf X^TUD^{-1}} \Rightarrow {\bf V}= \frac{1}{\sqrt{n-1}}{\bf QD^{-1}}$. \\

\hspace{-0.68cm} 3. ${\bf Z} = {\bf XV} = {\bf X}\frac{1}{\sqrt{n-1}}{\bf QD^{-1}} = \frac{1}{\sqrt{n-1}}{\bf XX^TUD^{-1}} = \frac{1}{\sqrt{n-1}}{\bf U{\Lambda}U^TU{\Lambda}^{-1/2}} = \frac{1}{\sqrt{n-1}}{\bf U{\Lambda}{\Lambda}^{-1/2}} = \frac{1}{\sqrt{n-1}}{\bf U{\Lambda}^{1/2}}$.

\section{Visualize PCA Results}
There is a really good \href{http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials}{article} providing practical guide for PCA. \\

\hspace{-0.68cm} You can also use the following link: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials

\end{document}