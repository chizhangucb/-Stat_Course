---
title: "PCA excercises"
author: "Rebecca Barter"
date: "9/14/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this week's exercise, we will be generating a fake dataset with a specified covariance structure, and then will be comparing how various different PCA methods perform on this dataset (they should all provide the same answer, but some will be much more efficient than others).


## Defining a covariance matrix

First, the following function defines a ($p \times p$) covariance matrix with decaying off diagonals such as in the following example:

$$
\textrm{Cov}(X)= \left[ \begin{array}{ccccccc}
1 & 0.4 & 0.3 & 0.2 & 0.1 & 0 & 0\\
0.4 & 1 & 0.4 & 0.3 & 0.2 & 0.1 & 0 \\
0.3 & 0.4 & 1 & 0.4 & 0.3 & 0.2 & 0.1\\
0.2 & 0.3 & 0.4 & 1 & 0.4 & 0.3 & 0.2\\
0.1 & 0.2 & 0.3 & 0.4 & 1 & 0.4 & 0.3 \\
0 & 0.1 & 0.2 & 0.3 & 0.4 & 1 & 0.4\\
0 & 0 & 0.1 & 0.2 & 0.3 & 0.4 & 1 
\end{array} \right]
$$

```{R echo = FALSE, warning = FALSE, message = FALSE}
library(Matrix)
library(irlba)
library(microbenchmark)
library(tidyverse)
library(GGally)
```


```{r}
GetCov <- function(p, m, max.cov = .5, sparse = T) {
  # Generate a covariance matrix with limited off-diagonal elements.
  #
  # Args:
  #   p: dimensionality of the cov mat
  #   m: number non-zero elements in each direction off the diagonal
  #   max.cov: maximum covariance between variables
  #   sparse: whether to use sparse data structure (Matrix vs matrix)
  #
  # Returns:
  #   A matrix with nonzeros close to the diagonal and zeros everywhere else
  #   Each row will look like 
  #       0 0 0 0 0 .1 .2 ... .9 1 .9  ... .2 .1 0 0 0 0 0

  # generate a sequence of covariances from 0 to max.cov
  r <- seq(from = max.cov, to = 0, length.out = m + 1)
  # remove the last element (0)
  r <- r[-length(r)]
  
  # generate an empty covariance matrix 
  # (defined as a sparse matrix if specified)
  if (sparse) {
    mat <- Matrix(0, nrow = p, ncol = p, sparse = T)
  } else {
    mat <- matrix(0, nrow = p,ncol = p)
  }
  
  # fill in the lower diagonal of the covariance marix with covariance values
  for (i in 1:length(r)) {
    # identify the off-diagnoal matrix indices 
    index <- seq(from = i + 1, by = p + 1, length.out = p - i )
    # fill in all lower off-diagonal indices with current covariance value
    mat[index] <- r[i]
  }
  
  # fill in the upper off-diagonals of the matrix
  mat <- mat + t(mat)
  # fill in the diagonal with 1s
  diag(mat) <- 1
  return(mat)
}
```

Let's define a covariance matrix with 10 variables $(p = 10)$, and with 4 off-diagonals ($m = 4$), and a maximum covariance of 0.8.

```{r}
# Model parameters.
p <- 10     # The dimension of each observation (number of variables).
m <- 4      # The number of off-diagonal covariance terms in each direction.
```

Notice that the sparse representation (below) replaces 0's with dots ($\cdot$).

```{r}
# Get the covariance matrix, set number of non zero off diagonals at 40
# First, use sparse matrices and check the size
cov_sparse <- GetCov(p, m, max.cov = 0.8, sparse = T)
cov_sparse
```

The standard non-sparse representation contains the same values but records 0s.
```{r}
# Now use dense matrices and check the size
cov_dense <- GetCov(p, m, max.cov = 0.8, sparse = F)
cov_dense
```


We will see below that computation with the sparse version can be much, much faster than computation with the dense version.

# Exercise 1 

**Let $n = 10000$. Use Cholesky decomposition to generate a dataset (an $n \times p$ matrix $X$) which has covariance matrix `cov_sparse` (compare the computation time between `cov_sparse` and `cov_dense`). Check that the covariance is correct. Generate a plot showing that adjacent columns are correlated with one another.**


# Exercise 2


**Use `scale()` to center and scale the columns. Calculate the principal components using the four functions `prcomp()`, `eigen()`, `svd()` and `irlba()`. Use microbenchmark to compare the speeds. Which method was the fastest? Confirm that they give the same answer. Note that both `svd()` and `irlba()` perform singular value decomposition, and you can restrict the functions to just look at the top five singular values.**


# Exercise 3

**Make a scree plot to decide how many principal components to choose**

# Exercise 4

**Run PCA analysis on the heptathlon data from the HSAUR package**

```{r}
# load in the heptathlon data
data("heptathlon", package = "HSAUR")
```

