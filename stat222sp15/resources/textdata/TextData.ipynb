{
 "metadata": {
  "name": "",
  "signature": "sha256:529c359b97286a1228bc5aaa120dd09d68d54f0d4477a77d3776ee46ef169799"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The material for this lecture is adapted from a few sources:\n",
      "*    the free book Natural Language Processing with Python, available from http://www.nltk.org/book\n",
      "*    tutuorials for the [gensim](http://radimrehurek.com/gensim/index.html) Python library\n",
      "\n",
      "These are great resources if you want to learn more."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLP refers to computations done on \"natural\" languages, i.e. naturally arising human languages such as English (NOT programming languages).\n",
      "\n",
      "Some applications:\n",
      "*    predicting word completion (e.g. for cellphones)\n",
      "*    computer translation (see http://translationparty.com/ for an illustration of current limitations)\n",
      "*    sentiment analysis\n",
      "*    search (which document is most relevant for a query)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Importing example data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll start with some data from the NLTK book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, string\n",
      "#nltk.download() # download book collection (need to do this once)\n",
      "from nltk.book import *\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see the name of the example texts\n",
      "text1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "<Text: Moby Dick by Herman Melville 1851>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "nltk.text.Text"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Later we'll learn how to create objects of this type. For now, just note we're working with something that looks like a list of words and punctuation. (These are called *tokens*.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[u'[',\n",
        " u'Moby',\n",
        " u'Dick',\n",
        " u'by',\n",
        " u'Herman',\n",
        " u'Melville',\n",
        " u'1851',\n",
        " u']',\n",
        " u'ETYMOLOGY',\n",
        " u'.']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text1) # in tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "260819"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Word useage: contexts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK has some very useful functions for examining how specific words are used; that is, examining their context."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "concordance: how are words used?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.concordance(\"monstrous\") # note that case is ignored"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 11 of 11 matches:\n",
        "ong the former , one was of a most monstrous size . ... This came towards us , \n",
        "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
        "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
        "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
        "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
        "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
        "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
        "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
        "ere to enter upon those still more monstrous stories of them which are to be fo\n",
        "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
        "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2.concordance(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 11 of 11 matches:\n",
        ". \" Now , Palmer , you shall see a monstrous pretty girl .\" He immediately went\n",
        "your sister is to marry him . I am monstrous glad of it , for then I shall have\n",
        "ou may tell your sister . She is a monstrous lucky girl to get him , upon my ho\n",
        "k how you will like them . Lucy is monstrous pretty , and so good humoured and \n",
        " Jennings , \" I am sure I shall be monstrous glad of Miss Marianne ' s company \n",
        " usual noisy cheerfulness , \" I am monstrous glad to see you -- sorry I could n\n",
        "t however , as it turns out , I am monstrous glad there was never any thing in \n",
        "so scornfully ! for they say he is monstrous fond of her , as well he may . I s\n",
        "possible that she should .\" \" I am monstrous glad of it . Good gracious ! I hav\n",
        "thing of the kind . So then he was monstrous happy , and talked on some time ab\n",
        "e very genteel people . He makes a monstrous deal of money , and they keep thei\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "similar: what words are used in a similar context?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2.similar(\"monstrous\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "very exceedingly so heartily a great good amazingly as sweet\n",
        "remarkably extremely vast\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "common_contexts: for two words used in similar contexts, see the contexts"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text2.common_contexts([\"monstrous\", \"very\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "a_pretty is_pretty a_lucky am_glad be_glad\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "collocations: see words often used together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
        "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
        "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
        "mate; white whale; ivory leg; one hand\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text8.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "would like; medium build; social drinker; quiet nights; non smoker;\n",
        "long term; age open; Would like; easy going; financially secure; fun\n",
        "times; similar interests; Age open; weekends away; poss rship; well\n",
        "presented; never married; single mum; permanent relationship; slim\n",
        "build\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "3. Word useage: frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's make an alphabetically sorted list of the unique tokens in the Book of Genesis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_tokens = sorted(set(text3))\n",
      "unique_tokens[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[u'!',\n",
        " u\"'\",\n",
        " u'(',\n",
        " u')',\n",
        " u',',\n",
        " u',)',\n",
        " u'.',\n",
        " u'.)',\n",
        " u':',\n",
        " u';',\n",
        " u';)',\n",
        " u'?',\n",
        " u'?)',\n",
        " u'A',\n",
        " u'Abel',\n",
        " u'Abelmizraim',\n",
        " u'Abidah',\n",
        " u'Abide',\n",
        " u'Abimael',\n",
        " u'Abimelech']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(unique_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "2789"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can measure \"lexical richness\" by dividing the number of unique tokens by the total number of tokens. Let's compare text1 (Moby Dick) to text6 (Monty Python)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(set(text1)) / len(text1)\n",
      "print len(set(text6)) / len(text6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0740628558502\n",
        "0.127659574468\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text3.count(\"begat\") # count a given token"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "67"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# note this is better for our purposes than the string version shown earlier\n",
      "print \"is this\".count(\"is\")\n",
      "print [\"is\", \"this\"].count(\"is\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\n",
        "1\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# percentage of text made up of specific word\n",
      "100 * text3.count(\"the\") / len(text3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "5.386024483960325"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK provides built-in support for working with frequency distributions (counts of each unique token)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1 = FreqDist(text1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print fdist1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<FreqDist with 19317 samples and 260819 outcomes>\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# extract count for a given token - compare with earlier\n",
      "fdist1[\"the\"] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "13721"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see most commonly occurring words; usually most consist of stop words\n",
      "fdist1.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "[(u',', 18713),\n",
        " (u'the', 13721),\n",
        " (u'.', 6862),\n",
        " (u'of', 6536),\n",
        " (u'and', 6024),\n",
        " (u'a', 4569),\n",
        " (u'to', 4542),\n",
        " (u';', 4072),\n",
        " (u'in', 3916),\n",
        " (u'that', 2982),\n",
        " (u\"'\", 2684),\n",
        " (u'-', 2552),\n",
        " (u'his', 2459),\n",
        " (u'it', 2209),\n",
        " (u'I', 2124),\n",
        " (u's', 1739),\n",
        " (u'is', 1695),\n",
        " (u'he', 1661),\n",
        " (u'with', 1659),\n",
        " (u'was', 1632)]"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "4. Identifying \"important\" words"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How can we identify \"important\" or \"interesting\" words in a text? One way of qualifying this is to find commonly used long words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show words that are > 7 characters long and occur more than 7 times\n",
      "fdist1 = FreqDist(text1)\n",
      "count_long = [(word, fdist1[word]) for word in set(text1) \n",
      "              if len(word) > 7 and fdist1[word] > 7]\n",
      "sorted(count_long, key=lambda el: -el[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[(u'Queequeg', 252),\n",
        " (u'Starbuck', 196),\n",
        " (u'something', 119),\n",
        " (u'Nantucket', 85),\n",
        " (u'sometimes', 81),\n",
        " (u'harpooneer', 77),\n",
        " (u'standing', 73),\n",
        " (u'whalemen', 71),\n",
        " (u'business', 67),\n",
        " (u'together', 64),\n",
        " (u'Leviathan', 64),\n",
        " (u'themselves', 59),\n",
        " (u'therefore', 56),\n",
        " (u'peculiar', 56),\n",
        " (u'harpooneers', 55),\n",
        " (u'Tashtego', 54),\n",
        " (u'thousand', 51),\n",
        " (u'particular', 49),\n",
        " (u'stranger', 48),\n",
        " (u'straight', 46),\n",
        " (u'touching', 45),\n",
        " (u'suddenly', 45),\n",
        " (u'whaleman', 44),\n",
        " (u'whatever', 44),\n",
        " (u'especially', 44),\n",
        " (u'creature', 42),\n",
        " (u'anything', 41),\n",
        " (u'wondrous', 41),\n",
        " (u'Steelkilt', 40),\n",
        " (u'carpenter', 39),\n",
        " (u'distance', 39),\n",
        " (u'concerning', 38),\n",
        " (u'gentlemen', 37),\n",
        " (u'forehead', 37),\n",
        " (u'bulwarks', 36),\n",
        " (u'Greenland', 35),\n",
        " (u'remained', 34),\n",
        " (u'previous', 34),\n",
        " (u'forecastle', 34),\n",
        " (u'thoughts', 34),\n",
        " (u'creatures', 34),\n",
        " (u'American', 34),\n",
        " (u'skeleton', 33),\n",
        " (u'completely', 33),\n",
        " (u'intervals', 32),\n",
        " (u'substance', 31),\n",
        " (u'darkness', 31),\n",
        " (u'leviathan', 31),\n",
        " (u'possible', 30),\n",
        " (u'harpoons', 30),\n",
        " (u'generally', 30),\n",
        " (u'instances', 30),\n",
        " (u'altogether', 29),\n",
        " (u'possibly', 29),\n",
        " (u'mariners', 29),\n",
        " (u'swimming', 29),\n",
        " (u'strangely', 29),\n",
        " (u'entirely', 29),\n",
        " (u'question', 29),\n",
        " (u'fishermen', 28),\n",
        " (u'considering', 28),\n",
        " (u'Nevertheless', 27),\n",
        " (u'landlord', 27),\n",
        " (u'certainly', 27),\n",
        " (u'precisely', 27),\n",
        " (u'somewhat', 27),\n",
        " (u'sideways', 27),\n",
        " (u'suspended', 27),\n",
        " (u'Fedallah', 27),\n",
        " (u'alongside', 27),\n",
        " (u'overboard', 27),\n",
        " (u'circumstances', 26),\n",
        " (u'enormous', 26),\n",
        " (u'followed', 26),\n",
        " (u'considerable', 26),\n",
        " (u'received', 26),\n",
        " (u'whiteness', 26),\n",
        " (u'yourself', 26),\n",
        " (u'otherwise', 26),\n",
        " (u'lightning', 26),\n",
        " (u'circumstance', 26),\n",
        " (u'complete', 25),\n",
        " (u'interval', 25),\n",
        " (u'thinking', 25),\n",
        " (u'cruising', 25),\n",
        " (u'floating', 24),\n",
        " (u'different', 24),\n",
        " (u'midnight', 24),\n",
        " (u'afterwards', 24),\n",
        " (u'following', 23),\n",
        " (u'somewhere', 23),\n",
        " (u'strength', 23),\n",
        " (u'striking', 23),\n",
        " (u'commanded', 23),\n",
        " (u'regarded', 23),\n",
        " (u'speaking', 23),\n",
        " (u'nevertheless', 23),\n",
        " (u'shipmates', 22),\n",
        " (u'Nantucketer', 22),\n",
        " (u'wonderful', 22),\n",
        " (u'answered', 22),\n",
        " (u'descried', 22),\n",
        " (u'windward', 22),\n",
        " (u'original', 21),\n",
        " (u'invested', 21),\n",
        " (u'terrible', 21),\n",
        " (u'magnitude', 21),\n",
        " (u'windlass', 21),\n",
        " (u'gentleman', 20),\n",
        " (u'directly', 20),\n",
        " (u'officers', 20),\n",
        " (u'attached', 20),\n",
        " (u'gigantic', 20),\n",
        " (u'according', 20),\n",
        " (u'sleeping', 20),\n",
        " (u'repeated', 20),\n",
        " (u'everything', 20),\n",
        " (u'slightest', 20),\n",
        " (u'separate', 20),\n",
        " (u'profound', 20),\n",
        " (u'solitary', 19),\n",
        " (u'Atlantic', 19),\n",
        " (u'ordinary', 19),\n",
        " (u'presented', 19),\n",
        " (u'vicinity', 19),\n",
        " (u'pointing', 19),\n",
        " (u'blacksmith', 19),\n",
        " (u'important', 19),\n",
        " (u'beginning', 19),\n",
        " (u'Meantime', 19),\n",
        " (u'concluded', 19),\n",
        " (u'captains', 19),\n",
        " (u'cannibal', 18),\n",
        " (u'revealed', 18),\n",
        " (u'Christian', 18),\n",
        " (u'observed', 18),\n",
        " (u'nameless', 18),\n",
        " (u'mentioned', 18),\n",
        " (u'instance', 18),\n",
        " (u'precious', 18),\n",
        " (u'advancing', 18),\n",
        " (u'interest', 18),\n",
        " (u'children', 18),\n",
        " (u'difference', 18),\n",
        " (u'prodigious', 18),\n",
        " (u'regularly', 18),\n",
        " (u'elephant', 17),\n",
        " (u'instantly', 17),\n",
        " (u'opposite', 17),\n",
        " (u'inserted', 17),\n",
        " (u'likewise', 17),\n",
        " (u'tomahawk', 17),\n",
        " (u'muttered', 17),\n",
        " (u'lowering', 17),\n",
        " (u'previously', 17),\n",
        " (u'understand', 17),\n",
        " (u'continually', 17),\n",
        " (u'captured', 17),\n",
        " (u'doubloon', 17),\n",
        " (u'spermaceti', 16),\n",
        " (u'originally', 16),\n",
        " (u'Meanwhile', 16),\n",
        " (u'binnacle', 16),\n",
        " (u'commander', 16),\n",
        " (u'civilized', 16),\n",
        " (u'actually', 16),\n",
        " (u'starboard', 16),\n",
        " (u'crossing', 16),\n",
        " (u'encountered', 16),\n",
        " (u'encounter', 16),\n",
        " (u'distinct', 16),\n",
        " (u'pleasant', 16),\n",
        " (u'dropping', 16),\n",
        " (u'scientific', 16),\n",
        " (u'uncommon', 16),\n",
        " (u'furnished', 16),\n",
        " (u'happened', 16),\n",
        " (u'breaking', 16),\n",
        " (u'numerous', 16),\n",
        " (u'stricken', 16),\n",
        " (u'hitherto', 16),\n",
        " (u'unaccountable', 16),\n",
        " (u'thousands', 16),\n",
        " (u'exclaimed', 15),\n",
        " (u'troubled', 15),\n",
        " (u'consider', 15),\n",
        " (u'marvellous', 15),\n",
        " (u'wrinkles', 15),\n",
        " (u'carrying', 15),\n",
        " (u'impossible', 15),\n",
        " (u'horizontal', 15),\n",
        " (u'invisible', 15),\n",
        " (u'employed', 15),\n",
        " (u'supposed', 15),\n",
        " (u'ourselves', 15),\n",
        " (u'exceedingly', 15),\n",
        " (u'position', 15),\n",
        " (u'merchant', 15),\n",
        " (u'disappeared', 15),\n",
        " (u'direction', 15),\n",
        " (u'latitudes', 15),\n",
        " (u'remember', 15),\n",
        " (u'watching', 15),\n",
        " (u'perilous', 15),\n",
        " (u'returned', 14),\n",
        " (u'wrinkled', 14),\n",
        " (u'continued', 14),\n",
        " (u'immortal', 14),\n",
        " (u'simultaneously', 14),\n",
        " (u'continual', 14),\n",
        " (u'whenever', 14),\n",
        " (u'terrific', 14),\n",
        " (u'probably', 14),\n",
        " (u'frequently', 14),\n",
        " (u'immediately', 14),\n",
        " (u'character', 14),\n",
        " (u'customary', 14),\n",
        " (u'silently', 14),\n",
        " (u'declared', 14),\n",
        " (u'critical', 14),\n",
        " (u'departed', 14),\n",
        " (u'considered', 14),\n",
        " (u'horrible', 14),\n",
        " (u'occasionally', 14),\n",
        " (u'perceived', 14),\n",
        " (u'tormented', 14),\n",
        " (u'anywhere', 14),\n",
        " (u'superior', 14),\n",
        " (u'slightly', 13),\n",
        " (u'sounding', 13),\n",
        " (u'breakfast', 13),\n",
        " (u'ponderous', 13),\n",
        " (u'pictures', 13),\n",
        " (u'dangerous', 13),\n",
        " (u'shoulders', 13),\n",
        " (u'swinging', 13),\n",
        " (u'belonged', 13),\n",
        " (u'indirectly', 13),\n",
        " (u'trowsers', 13),\n",
        " (u'practical', 13),\n",
        " (u'individual', 13),\n",
        " (u'presently', 13),\n",
        " (u'latitude', 13),\n",
        " (u'resolved', 13),\n",
        " (u'remarkable', 13),\n",
        " (u'swallowed', 13),\n",
        " (u'quantity', 13),\n",
        " (u'subsequent', 13),\n",
        " (u'tapering', 12),\n",
        " (u'bursting', 12),\n",
        " (u'downwards', 12),\n",
        " (u'prepared', 12),\n",
        " (u'contrast', 12),\n",
        " (u'murmured', 12),\n",
        " (u'naturally', 12),\n",
        " (u'ambergris', 12),\n",
        " (u'enchanted', 12),\n",
        " (u'external', 12),\n",
        " (u'centuries', 12),\n",
        " (u'conscience', 12),\n",
        " (u'unearthly', 12),\n",
        " (u'sweeping', 12),\n",
        " (u'meanwhile', 12),\n",
        " (u'exceeding', 12),\n",
        " (u'connected', 12),\n",
        " (u'desperate', 12),\n",
        " (u'reaching', 12),\n",
        " (u'doubtless', 12),\n",
        " (u'shoulder', 12),\n",
        " (u'involuntarily', 12),\n",
        " (u'interior', 12),\n",
        " (u'countenance', 12),\n",
        " (u'mountain', 12),\n",
        " (u'stranded', 12),\n",
        " (u'knowledge', 12),\n",
        " (u'greatest', 12),\n",
        " (u'glancing', 12),\n",
        " (u'contrary', 12),\n",
        " (u'NANTUCKET', 11),\n",
        " (u'demanded', 11),\n",
        " (u'fastened', 11),\n",
        " (u'spiritual', 11),\n",
        " (u'infallibly', 11),\n",
        " (u'peculiarities', 11),\n",
        " (u'concluding', 11),\n",
        " (u'commotion', 11),\n",
        " (u'lengthwise', 11),\n",
        " (u'elevated', 11),\n",
        " (u'elsewhere', 11),\n",
        " (u'indifferent', 11),\n",
        " (u'convenient', 11),\n",
        " (u'additional', 11),\n",
        " (u'receiving', 11),\n",
        " (u'valuable', 11),\n",
        " (u'temporary', 11),\n",
        " (u'experienced', 11),\n",
        " (u'spouting', 11),\n",
        " (u'vengeance', 11),\n",
        " (u'features', 11),\n",
        " (u'reference', 11),\n",
        " (u'monomaniac', 11),\n",
        " (u'fashioned', 11),\n",
        " (u'perpendicular', 11),\n",
        " (u'naturalists', 11),\n",
        " (u'whispered', 11),\n",
        " (u'invariably', 11),\n",
        " (u'helmsman', 11),\n",
        " (u'significant', 11),\n",
        " (u'appearance', 11),\n",
        " (u'straightway', 11),\n",
        " (u'excellent', 11),\n",
        " (u'blackness', 11),\n",
        " (u'shipmate', 11),\n",
        " (u'advanced', 11),\n",
        " (u'steadily', 11),\n",
        " (u'indispensable', 11),\n",
        " (u'Therefore', 11),\n",
        " (u'barbaric', 11),\n",
        " (u'beholding', 11),\n",
        " (u'strongly', 11),\n",
        " (u'included', 11),\n",
        " (u'prolonged', 11),\n",
        " (u'hoisting', 11),\n",
        " (u'vocation', 11),\n",
        " (u'Guernsey', 11),\n",
        " (u'uncertain', 10),\n",
        " (u'bringing', 10),\n",
        " (u'remotest', 10),\n",
        " (u'foremost', 10),\n",
        " (u'superstitious', 10),\n",
        " (u'leviathans', 10),\n",
        " (u'impressions', 10),\n",
        " (u'appalling', 10),\n",
        " (u'experience', 10),\n",
        " (u'possession', 10),\n",
        " (u'retained', 10),\n",
        " (u'occurred', 10),\n",
        " (u'steering', 10),\n",
        " (u'mainmast', 10),\n",
        " (u'grinning', 10),\n",
        " (u'striving', 10),\n",
        " (u'Nantucketers', 10),\n",
        " (u'mysterious', 10),\n",
        " (u'discovered', 10),\n",
        " (u'moreover', 10),\n",
        " (u'downward', 10),\n",
        " (u'comrades', 10),\n",
        " (u'velocity', 10),\n",
        " (u'throwing', 10),\n",
        " (u'Jeroboam', 10),\n",
        " (u'remaining', 10),\n",
        " (u'venerable', 10),\n",
        " (u'ignorant', 10),\n",
        " (u'uplifted', 10),\n",
        " (u'curiosity', 10),\n",
        " (u'surprise', 10),\n",
        " (u'ignorance', 10),\n",
        " (u'appeared', 10),\n",
        " (u'entitled', 10),\n",
        " (u'afternoon', 10),\n",
        " (u'headsman', 10),\n",
        " (u'elephants', 10),\n",
        " (u'murderous', 10),\n",
        " (u'carefully', 10),\n",
        " (u'smallest', 10),\n",
        " (u'yesterday', 10),\n",
        " (u'Moreover', 10),\n",
        " (u'stripped', 10),\n",
        " (u'occasion', 10),\n",
        " (u'inferior', 10),\n",
        " (u'domestic', 10),\n",
        " (u'consternation', 10),\n",
        " (u'compasses', 10),\n",
        " (u'supplied', 10),\n",
        " (u'Japanese', 10),\n",
        " (u'discovery', 10),\n",
        " (u'Porpoise', 10),\n",
        " (u'landsmen', 10),\n",
        " (u'gradually', 10),\n",
        " (u'eastward', 10),\n",
        " (u'articles', 10),\n",
        " (u'monstrous', 10),\n",
        " (u'hovering', 10),\n",
        " (u'violently', 9),\n",
        " (u'extremity', 9),\n",
        " (u'detached', 9),\n",
        " (u'considerably', 9),\n",
        " (u'apparition', 9),\n",
        " (u'expression', 9),\n",
        " (u'combined', 9),\n",
        " (u'intended', 9),\n",
        " (u'revolving', 9),\n",
        " (u'shuddering', 9),\n",
        " (u'hereafter', 9),\n",
        " (u'approaching', 9),\n",
        " (u'monsters', 9),\n",
        " (u'influence', 9),\n",
        " (u'occasional', 9),\n",
        " (u'cannibals', 9),\n",
        " (u'involved', 9),\n",
        " (u'singular', 9),\n",
        " (u'sufficiently', 9),\n",
        " (u'traveller', 9),\n",
        " (u'withstand', 9),\n",
        " (u'temporarily', 9),\n",
        " (u'accursed', 9),\n",
        " (u'daylight', 9),\n",
        " (u'unspeakable', 9),\n",
        " (u'sparkling', 9),\n",
        " (u'larboard', 9),\n",
        " (u'accounted', 9),\n",
        " (u'comfortable', 9),\n",
        " (u'independent', 9),\n",
        " (u'seemingly', 9),\n",
        " (u'intently', 9),\n",
        " (u'strained', 9),\n",
        " (u'intolerable', 9),\n",
        " (u'confidential', 9),\n",
        " (u'relieved', 9),\n",
        " (u'forgotten', 9),\n",
        " (u'Commodore', 9),\n",
        " (u'properly', 9),\n",
        " (u'Scoresby', 9),\n",
        " (u'projecting', 9),\n",
        " (u'vigorous', 9),\n",
        " (u'principle', 9),\n",
        " (u'particulars', 9),\n",
        " (u'recognised', 9),\n",
        " (u'occupied', 9),\n",
        " (u'sensible', 9),\n",
        " (u'devilish', 9),\n",
        " (u'malicious', 9),\n",
        " (u'stretched', 9),\n",
        " (u'everlasting', 9),\n",
        " (u'movement', 9),\n",
        " (u'scattered', 9),\n",
        " (u'throughout', 9),\n",
        " (u'reserved', 9),\n",
        " (u'apparently', 9),\n",
        " (u'tremendous', 9),\n",
        " (u'mystical', 9),\n",
        " (u'destroyed', 9),\n",
        " (u'Mediterranean', 9),\n",
        " (u'stopping', 9),\n",
        " (u'descending', 9),\n",
        " (u'succeeded', 9),\n",
        " (u'tumultuous', 9),\n",
        " (u'abounding', 9),\n",
        " (u'tambourine', 9),\n",
        " (u'accounts', 9),\n",
        " (u'directions', 9),\n",
        " (u'presumed', 9),\n",
        " (u'freighted', 9),\n",
        " (u'activity', 9),\n",
        " (u'attention', 9),\n",
        " (u'contrasting', 9),\n",
        " (u'inclined', 9),\n",
        " (u'dripping', 9),\n",
        " (u'imperial', 9),\n",
        " (u'levelled', 9),\n",
        " (u'Sebastian', 9),\n",
        " (u'thrusting', 9),\n",
        " (u'incredible', 9),\n",
        " (u'whalebone', 8),\n",
        " (u'specially', 8),\n",
        " (u'conscious', 8),\n",
        " (u'contrivances', 8),\n",
        " (u'lingering', 8),\n",
        " (u'elements', 8),\n",
        " (u'beautiful', 8),\n",
        " (u'operation', 8),\n",
        " (u'taffrail', 8),\n",
        " (u'connexion', 8),\n",
        " (u'measureless', 8),\n",
        " (u'hammered', 8),\n",
        " (u'concerned', 8),\n",
        " (u'consideration', 8),\n",
        " (u'transparent', 8),\n",
        " (u'conclude', 8),\n",
        " (u'Bulkington', 8),\n",
        " (u'compared', 8),\n",
        " (u'perpendicularly', 8),\n",
        " (u'glittering', 8),\n",
        " (u'clinging', 8),\n",
        " (u'reasonable', 8),\n",
        " (u'miserable', 8),\n",
        " (u'traditions', 8),\n",
        " (u'slouched', 8),\n",
        " (u'preliminary', 8),\n",
        " (u'vitality', 8),\n",
        " (u'bestowed', 8),\n",
        " (u'harpooned', 8),\n",
        " (u'inscrutable', 8),\n",
        " (u'Heidelburgh', 8),\n",
        " (u'moonlight', 8),\n",
        " (u'surrounded', 8),\n",
        " (u'coloured', 8),\n",
        " (u'Wherefore', 8),\n",
        " (u'unnatural', 8),\n",
        " (u'fountain', 8),\n",
        " (u'infinite', 8),\n",
        " (u'obedience', 8),\n",
        " (u'described', 8),\n",
        " (u'authority', 8),\n",
        " (u'melancholy', 8),\n",
        " (u'including', 8),\n",
        " (u'popularly', 8),\n",
        " (u'infernal', 8),\n",
        " (u'scarcely', 8),\n",
        " (u'mistaken', 8),\n",
        " (u'phantoms', 8),\n",
        " (u'instinct', 8),\n",
        " (u'universal', 8),\n",
        " (u'solemnly', 8),\n",
        " (u'Consider', 8),\n",
        " (u'perceive', 8),\n",
        " (u'hundreds', 8),\n",
        " (u'tattooed', 8),\n",
        " (u'historical', 8),\n",
        " (u'Canallers', 8),\n",
        " (u'interesting', 8),\n",
        " (u'distinctly', 8),\n",
        " (u'eventually', 8),\n",
        " (u'starting', 8),\n",
        " (u'archangel', 8),\n",
        " (u'particularly', 8),\n",
        " (u'withdrawing', 8),\n",
        " (u'motionless', 8),\n",
        " (u'bowsprit', 8),\n",
        " (u'snatching', 8),\n",
        " (u'intensity', 8),\n",
        " (u'judgment', 8),\n",
        " (u'proceeds', 8),\n",
        " (u'comparatively', 8),\n",
        " (u'published', 8),\n",
        " (u'hopeless', 8),\n",
        " (u'containing', 8),\n",
        " (u'specific', 8),\n",
        " (u'backward', 8),\n",
        " (u'obliquely', 8),\n",
        " (u'gunwales', 8),\n",
        " (u'fearless', 8),\n",
        " (u'belonging', 8),\n",
        " (u'observing', 8),\n",
        " (u'vertebrae', 8),\n",
        " (u'mechanically', 8),\n",
        " (u'mountains', 8),\n",
        " (u'narrative', 8),\n",
        " (u'submerged', 8),\n",
        " (u'material', 8),\n",
        " (u'respects', 8),\n",
        " (u'landsman', 8),\n",
        " (u'grandeur', 8),\n",
        " (u'intention', 8),\n",
        " (u'eagerness', 8)]"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Approaching this from the opposite direction, we might *remove* commonly occuring short words. These are also known as *stop words*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stopwords = nltk.corpus.stopwords.words('english')\n",
      "stopwords[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "[u'i',\n",
        " u'me',\n",
        " u'my',\n",
        " u'myself',\n",
        " u'we',\n",
        " u'our',\n",
        " u'ours',\n",
        " u'ourselves',\n",
        " u'you',\n",
        " u'your']"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Find most frequently occurring words, removing stop words first\n",
      "content1 = [word for word in text1 if word.lower() not in stopwords]\n",
      "fdist1 = FreqDist(content1)\n",
      "fdist1.most_common(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "[(u',', 18713),\n",
        " (u'.', 6862),\n",
        " (u';', 4072),\n",
        " (u\"'\", 2684),\n",
        " (u'-', 2552),\n",
        " (u'\"', 1478),\n",
        " (u'!', 1269),\n",
        " (u'--', 1070),\n",
        " (u'whale', 906),\n",
        " (u'one', 889),\n",
        " (u'?', 637),\n",
        " (u'like', 624),\n",
        " (u'upon', 538),\n",
        " (u'man', 508),\n",
        " (u'ship', 507),\n",
        " (u'Ahab', 501),\n",
        " (u'.\"', 489),\n",
        " (u'ye', 460),\n",
        " (u'old', 436),\n",
        " (u'sea', 433)]"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll come back to the question of how to remove tokens that are nothing but punctuation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "5. Working with raw text data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll grab some text from the web. Project Gutenberg has an online collection of free ebooks in various formats, including plain text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "\n",
      "# link for Crime and Punishment\n",
      "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
      "\n",
      "response = urllib.urlopen(url)\n",
      "raw = response.read().decode('utf8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "unicode"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Length - in characters, not tokens\n",
      "len(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "1176896"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "u'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Crime and Punishment\\r\\n\\r\\nAuthor: Fyodor Dostoevsky\\r\\n\\r\\nRelease Date: March 28, 2006 [EBook #2554]\\r\\n[Last updated: November 15, 2011]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nChar'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Manually searching for the content"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I opened the link from the Project Gutenburg website to look at the text we just downloaded. There's an introduction I don't want to include. Scrolling down, I see that the book begins with the words \"PART I.\" There's also some copyright information at the end, following the words \"End of Project Gutenberg's Crime and Punishment, by Fyodor Dostoevsky.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw.find(\"PART I\") # first instance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "5338"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw[5338:5500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "u'PART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early in July a young man came out of\\r\\nthe garret in which he lodged in S. Place and walked slowly, as '"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw.rfind(\"End of Project Gutenberg's Crime\") # last instance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "1157746"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw[1157600:1157746]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "u'd into another, of his initiation into a new unknown life.\\r\\nThat might be the subject of a new story, but our present story is\\r\\nended.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n'"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = raw[5338:1157746]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tokenizers divide strings into lists of substrings. Usually we do this because we want to split a string into sentences or words. This topic is complex and NLTK has some built-in functions we can use, without delving too much into the algorithms behind them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "u'PART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early in July a young man came out of\\r\\nthe garret in which he lodged in S. Place and walked slowly, as though\\r\\nin hesitation, towards K. bridge.\\r\\n\\r\\nHe had successfully avoided meeting his landlady on the staircase. His\\r\\ngarret was under the roof of a high, five-storied house and was more\\r\\nlike a cupboard than a room. The landlady who provided him with garret,\\r\\ndinners, and attendance, lived on the floor below, and every time\\r\\nhe went out'"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One simple way to tokenize is with split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# default separator is any whitespace\n",
      "raw[:500].split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[u'PART',\n",
        " u'I',\n",
        " u'CHAPTER',\n",
        " u'I',\n",
        " u'On',\n",
        " u'an',\n",
        " u'exceptionally',\n",
        " u'hot',\n",
        " u'evening',\n",
        " u'early',\n",
        " u'in',\n",
        " u'July',\n",
        " u'a',\n",
        " u'young',\n",
        " u'man',\n",
        " u'came',\n",
        " u'out',\n",
        " u'of',\n",
        " u'the',\n",
        " u'garret',\n",
        " u'in',\n",
        " u'which',\n",
        " u'he',\n",
        " u'lodged',\n",
        " u'in',\n",
        " u'S.',\n",
        " u'Place',\n",
        " u'and',\n",
        " u'walked',\n",
        " u'slowly,',\n",
        " u'as',\n",
        " u'though',\n",
        " u'in',\n",
        " u'hesitation,',\n",
        " u'towards',\n",
        " u'K.',\n",
        " u'bridge.',\n",
        " u'He',\n",
        " u'had',\n",
        " u'successfully',\n",
        " u'avoided',\n",
        " u'meeting',\n",
        " u'his',\n",
        " u'landlady',\n",
        " u'on',\n",
        " u'the',\n",
        " u'staircase.',\n",
        " u'His',\n",
        " u'garret',\n",
        " u'was',\n",
        " u'under',\n",
        " u'the',\n",
        " u'roof',\n",
        " u'of',\n",
        " u'a',\n",
        " u'high,',\n",
        " u'five-storied',\n",
        " u'house',\n",
        " u'and',\n",
        " u'was',\n",
        " u'more',\n",
        " u'like',\n",
        " u'a',\n",
        " u'cupboard',\n",
        " u'than',\n",
        " u'a',\n",
        " u'room.',\n",
        " u'The',\n",
        " u'landlady',\n",
        " u'who',\n",
        " u'provided',\n",
        " u'him',\n",
        " u'with',\n",
        " u'garret,',\n",
        " u'dinners,',\n",
        " u'and',\n",
        " u'attendance,',\n",
        " u'lived',\n",
        " u'on',\n",
        " u'the',\n",
        " u'floor',\n",
        " u'below,',\n",
        " u'and',\n",
        " u'every',\n",
        " u'time',\n",
        " u'he',\n",
        " u'went',\n",
        " u'out']"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "type(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "list"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "250031"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokens[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'PART', u'I', u'CHAPTER', u'I', u'On', u'an', u'exceptionally', u'hot', u'evening', u'early', u'in', u'July', u'a', u'young', u'man', u'came', u'out', u'of', u'the', u'garret', u'in', u'which', u'he', u'lodged', u'in', u'S.', u'Place', u'and', u'walked', u'slowly', u',', u'as', u'though', u'in', u'hesitation', u',', u'towards', u'K.', u'bridge', u'.', u'He', u'had', u'successfully', u'avoided', u'meeting', u'his', u'landlady', u'on', u'the', u'staircase']\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The word_tokenize function is based on the [Treebank tokenization algorithm](http://www.cis.upenn.edu/~treebank/tokenization.html). One advantage of this algorithm is that it handles contractions in an appropriate way, which is tricky to do for all cases using regular expressions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tokenize(\"I've been to the U.S. twice. I don't plan to go back.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "['I',\n",
        " \"'ve\",\n",
        " 'been',\n",
        " 'to',\n",
        " 'the',\n",
        " 'U.S.',\n",
        " 'twice',\n",
        " '.',\n",
        " 'I',\n",
        " 'do',\n",
        " \"n't\",\n",
        " 'plan',\n",
        " 'to',\n",
        " 'go',\n",
        " 'back',\n",
        " '.']"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import RegexpTokenizer\n",
      "tokenizer = RegexpTokenizer(r'\\w+') # one or more word characters\n",
      "tokenizer.tokenize(\"I've been to the U.S. twice. I don't plan to go back.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "['I',\n",
        " 've',\n",
        " 'been',\n",
        " 'to',\n",
        " 'the',\n",
        " 'U',\n",
        " 'S',\n",
        " 'twice',\n",
        " 'I',\n",
        " 'don',\n",
        " 't',\n",
        " 'plan',\n",
        " 'to',\n",
        " 'go',\n",
        " 'back']"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One refinement we might want to make is to remove the tokens that consist *only* of punctuation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "string.punctuation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def all_punct(x):\n",
      "    return(all([char in string.punctuation for char in x]))\n",
      "\n",
      "def my_tokenize(text):\n",
      "    init_words = word_tokenize(text)\n",
      "    return([w for w in init_words if not all_punct(w)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = my_tokenize(raw)\n",
      "print tokens[:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'PART', u'I', u'CHAPTER', u'I', u'On', u'an', u'exceptionally', u'hot', u'evening', u'early', u'in', u'July', u'a', u'young', u'man', u'came', u'out', u'of', u'the', u'garret', u'in', u'which', u'he', u'lodged', u'in', u'S.', u'Place', u'and', u'walked', u'slowly', u'as', u'though', u'in', u'hesitation', u'towards', u'K.', u'bridge', u'He', u'had', u'successfully', u'avoided', u'meeting', u'his', u'landlady', u'on', u'the', u'staircase', u'His', u'garret', u'was']\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Exercise:\n",
      "1.    Write a new function called any_punct that checks whether a string contains any punctuation.\n",
      "2.    Use this function to create a set of tokens containing punctuation from the example above. Do you see any examples of tokens that suggest further improvements we could make?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Creating an nltk.Text object"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can create an nltk.Text object and use all the NLTK functions for processing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "type(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "nltk.text.Text"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print text[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'PART', u'I', u'CHAPTER', u'I', u'On', u'an', u'exceptionally', u'hot', u'evening', u'early', u'in', u'July', u'a', u'young', u'man', u'came', u'out', u'of', u'the', u'garret']\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
        "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
        "woman; Porfiry Petrovitch; Amalia Ivanovna; great deal; Nikodim\n",
        "Fomitch; young man; Ilya Petrovitch; n't know; Dmitri Prokofitch;\n",
        "Andrey Semyonovitch; Hay Market; Good heavens; police station\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = [w for w in text if w.lower() not in stopwords]\n",
      "fdist = FreqDist(content)\n",
      "fdist.most_common(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "[(u\"'s\", 1620),\n",
        " (u\"n't\", 1065),\n",
        " (u'Raskolnikov', 778),\n",
        " (u'would', 587),\n",
        " (u'one', 576),\n",
        " (u'know', 527),\n",
        " (u'could', 521),\n",
        " (u'said', 517),\n",
        " (u'man', 470),\n",
        " (u'like', 449)]"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "6. Vector representations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Much of advanced text processing is based on creating a vector representation of each text. Think of each element of the vector being a real number representing the answer to a specific question. \n",
      "\n",
      "For example, we could have a dictionary of all possible words and then a long vector counting the number of times each word occurs in a text. Note that this vector would be *sparse*, i.e. containing many zeroes. Efficient implementatations of text-mining algorithms take advantage of this sparsity.\n",
      "\n",
      "For working with efficient vector representations, we can use the [gensim](http://radimrehurek.com/gensim/index.html) Python library. I'm going to switch over now to using a simple example from one of the gensim tutorials.\n",
      "\n",
      "Imagine that the following are titles of academic papers. The goal is to return the best matching paper for a particular search string."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "documents = [\"Human machine interface for lab abc computer applications\",\n",
      "              \"A survey of user opinion of computer system response time\",\n",
      "              \"The EPS user interface management system\",\n",
      "              \"System and human system engineering testing of EPS\",\n",
      "              \"Relation of user perceived response time to error measurement\",\n",
      "              \"The generation of random binary unordered trees\",\n",
      "              \"The intersection graph of paths in trees\",\n",
      "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
      "              \"Graph minors A survey\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove common words and tokenize\n",
      "stoplist = set('for a of the and to in'.split()) # an abbreviated list of stop words\n",
      "texts = [[word for word in document.lower().split() \n",
      "          if word not in stoplist]\n",
      "          for document in documents]\n",
      "texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 53,
       "text": [
        "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
        " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
        " ['eps', 'user', 'interface', 'management', 'system'],\n",
        " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
        " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
        " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
        " ['intersection', 'graph', 'paths', 'trees'],\n",
        " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
        " ['graph', 'minors', 'survey']]"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# find the unique words\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(35 unique tokens: [u'minors', u'generation', u'testing', u'iv', u'engineering']...)\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dictionary.token2id # numbers represent ids, not counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'minors': 30, u'generation': 22, u'testing': 16, u'iv': 29, u'engineering': 15, u'computer': 2, u'relation': 20, u'human': 3, u'measurement': 18, u'unordered': 25, u'binary': 21, u'abc': 0, u'ordering': 31, u'graph': 26, u'system': 10, u'machine': 6, u'quasi': 32, u'random': 23, u'paths': 28, u'error': 17, u'trees': 24, u'lab': 5, u'applications': 1, u'management': 14, u'user': 12, u'interface': 4, u'intersection': 27, u'response': 8, u'perceived': 19, u'widths': 34, u'well': 33, u'eps': 13, u'survey': 9, u'time': 11, u'opinion': 7}\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for a new string, convert to \"bag of words\" representation using the dictionary\n",
      "dictionary.doc2bow(\"human computer interaction survey computer\".split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "[(2, 2), (3, 1), (9, 1)]"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the word \"interaction\" is not in the dictionary and is ignored."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert all documents\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
        " [(2, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
        " [(4, 1), (10, 1), (12, 1), (13, 1), (14, 1)],\n",
        " [(3, 1), (10, 2), (13, 1), (15, 1), (16, 1)],\n",
        " [(8, 1), (11, 1), (12, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
        " [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1)],\n",
        " [(24, 1), (26, 1), (27, 1), (28, 1)],\n",
        " [(24, 1), (26, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)],\n",
        " [(9, 1), (26, 1), (30, 1)]]"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A *term-document matrix* has rows representing words/tokens and columns representing documents. Each element counts the number of times a particular word occurs in a particular document. We can think of the corpus object above as representing the term-document matrix in a format that discards all the zero entries.\n",
      "\n",
      "It is common to divide each entry by a function of the number of times the word occurs in the entire corpus (collection of documents). This calculation is called TF-IDF, which stands for \"term frequency x inverse document frequency.\"\n",
      "\n",
      "Let's see the options by looking at the help for models.TfidfModel in gensim."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "models.TfidfModel?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = models.TfidfModel(corpus, normalize=True)\n",
      "corpus_tfidf = tfidf[corpus]\n",
      "for doc in corpus_tfidf:\n",
      "    print(doc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.4301019571350565), (1, 0.4301019571350565), (2, 0.2944198962221451), (3, 0.2944198962221451), (4, 0.2944198962221451), (5, 0.4301019571350565), (6, 0.4301019571350565)]\n",
        "[(2, 0.3726494271826947), (7, 0.5443832091958983), (8, 0.3726494271826947), (9, 0.3726494271826947), (10, 0.27219160459794917), (11, 0.3726494271826947), (12, 0.27219160459794917)]\n",
        "[(4, 0.438482464916089), (10, 0.32027755044706185), (12, 0.32027755044706185), (13, 0.438482464916089), (14, 0.6405551008941237)]\n",
        "[(3, 0.3449874408519962), (10, 0.5039733231394895), (13, 0.3449874408519962), (15, 0.5039733231394895), (16, 0.5039733231394895)]\n",
        "[(8, 0.30055933182961736), (11, 0.30055933182961736), (12, 0.21953536176370683), (17, 0.43907072352741366), (18, 0.43907072352741366), (19, 0.43907072352741366), (20, 0.43907072352741366)]\n",
        "[(21, 0.48507125007266594), (22, 0.48507125007266594), (23, 0.48507125007266594), (24, 0.24253562503633297), (25, 0.48507125007266594)]\n",
        "[(24, 0.31622776601683794), (26, 0.31622776601683794), (27, 0.6324555320336759), (28, 0.6324555320336759)]\n",
        "[(24, 0.20466057569885868), (26, 0.20466057569885868), (29, 0.40932115139771735), (30, 0.2801947048062438), (31, 0.40932115139771735), (32, 0.40932115139771735), (33, 0.40932115139771735), (34, 0.40932115139771735)]\n",
        "[(9, 0.6282580468670046), (26, 0.45889394536615247), (30, 0.6282580468670046)]\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we're ready to implement the search. First we need to convert our query to the TF-IDF representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "doc = \"Human computer interaction\"\n",
      "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
      "vec_tfidf = tfidf[vec_bow]\n",
      "print(vec_tfidf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(2, 0.7071067811865476), (3, 0.7071067811865476)]\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need a way of determining which of the article titles are \"closest\" to our query. Think of our vectors in high-dimensional space. We'll use the cosine of the angle between each pair of vectors as our similarity metric."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = similarities.MatrixSimilarity(corpus_tfidf)\n",
      "sims = index[vec_tfidf] # perform a similarity query against the corpus\n",
      "print(list(enumerate(sims))) # print (document_number, document_similarity) 2-tuples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.4163726), (1, 0.26350293), (2, 0.0), (3, 0.24394296), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(enumerate(sims), key=lambda item: -item[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 61,
       "text": [
        "[(0, 0.4163726),\n",
        " (1, 0.26350293),\n",
        " (3, 0.24394296),\n",
        " (2, 0.0),\n",
        " (4, 0.0),\n",
        " (5, 0.0),\n",
        " (6, 0.0),\n",
        " (7, 0.0),\n",
        " (8, 0.0)]"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Various other transformations of the basic bag-of-words representation have been proposed. For example, Latent Semantic Indexing (LSI) is based on taking the SVD of either the term-document matrix or the TF-IDF matrix. See the list of avaiable transformations in gensim [here](http://radimrehurek.com/gensim/tut2.html)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}